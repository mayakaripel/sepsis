{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10719274,"sourceType":"datasetVersion","datasetId":6644419}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:13:43.361055Z\",\"iopub.execute_input\":\"2025-03-01T05:13:43.361288Z\",\"iopub.status.idle\":\"2025-03-01T05:13:43.671505Z\",\"shell.execute_reply.started\":\"2025-03-01T05:13:43.361268Z\",\"shell.execute_reply\":\"2025-03-01T05:13:43.670740Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [markdown]\n# # Install Libraries and Setup Logging\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:13:43.672386Z\",\"iopub.execute_input\":\"2025-03-01T05:13:43.672738Z\",\"iopub.status.idle\":\"2025-03-01T05:14:05.562783Z\",\"shell.execute_reply.started\":\"2025-03-01T05:13:43.672709Z\",\"shell.execute_reply\":\"2025-03-01T05:14:05.561775Z\"}}\n# Cell 1: Install Libraries and Setup Logging\nimport subprocess\n\ntry:\n    print(\"Installing dask-ml and downgrading scikit-learn if necessary...\")\n    subprocess.check_call([\"pip\", \"install\", \"dask-ml\", \"--upgrade\"])\n    subprocess.check_call([\"pip\", \"install\", \"scikit-learn==1.1.3\"])  # Important:  Downgrade scikit-learn\n    print(\"Libraries installed successfully.\")\nexcept Exception as e:\n    print(f\"Error during library installation: {e}\")\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.metrics import precision_recall_curve, roc_auc_score, classification_report, make_scorer, roc_curve\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(\"Libraries imported successfully.\")\n\n# %% [markdown]\n# # Define Helper Functions\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:05.563775Z\",\"iopub.execute_input\":\"2025-03-01T05:14:05.564492Z\",\"iopub.status.idle\":\"2025-03-01T05:14:05.576140Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:05.564456Z\",\"shell.execute_reply\":\"2025-03-01T05:14:05.575262Z\"}}\n# Cell 2: Define Helper Functions\ndef safe_knn_impute(df, n_neighbors=5):\n    \"\"\"Imputes missing values using KNN but ensures no NaNs remain.\"\"\"\n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    imputed_data = imputer.fit_transform(df)\n    return pd.DataFrame(imputed_data, columns=df.columns)\n\ndef create_lagged_features(df, column, lags, groupby_column='Patient_ID'):\n    \"\"\"Creates lagged features for a given column.\"\"\"\n    df_with_lags = df.copy()\n    for lag in lags:\n        try:\n            df_with_lags[f'{column}_lag_{lag}'] = df_with_lags.groupby(groupby_column)[column].shift(lag).fillna(method='bfill')\n        except KeyError as e:\n            print(f\"Column {e} not found: {column}.  Skipping feature creation.\")\n            return df  # Return original DataFrame if a column is missing\n        except Exception as e:\n            print(f\"Error creating lag features for {column}: {e}\")\n            return df  #Return original dataframe.\n    return df_with_lags\n\ndef optimize_threshold(y_true, y_probs):\n    \"\"\"Finds the optimal threshold for classification.\"\"\"\n    precisions, recalls, thresholds = precision_recall_curve(y_true, y_probs)\n    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n    best_threshold = thresholds[np.argmax(f1_scores[:-1])]\n    return best_threshold\n\n# Custom ROC AUC scoring function to handle potential errors\ndef custom_roc_auc(y_true, y_pred):\n    \"\"\"Calculates ROC AUC but handles errors.\"\"\"\n    try:\n        return roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return 0.5  # Return a neutral score in case of errors\n\n# %% [markdown]\n# # Load and Preprocess Data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:05.577101Z\",\"iopub.execute_input\":\"2025-03-01T05:14:05.577389Z\",\"iopub.status.idle\":\"2025-03-01T05:14:53.274717Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:05.577364Z\",\"shell.execute_reply\":\"2025-03-01T05:14:53.274028Z\"}}\n# Cell 3: Load and Preprocess Data\nlogging.info(\"Loading dataset...\")\ntry:\n    # Load the data as a Dask DataFrame\n    df = dd.read_csv('/kaggle/input/sepsis/Dataset.csv')\n\n    # Handle HospAdmTime Negative Values\n    if 'HospAdmTime' in df.columns:\n        df['HospAdmTime'] = df['HospAdmTime'].abs()\n\nexcept FileNotFoundError:\n    print(\"Error: Dataset.csv not found.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error reading CSV: {e}\")\n    exit()\n\n# Drop columns with more than 90% missing values\nmissing_threshold = 0.9\nmissing_percent = df.isna().sum().compute() / len(df)  # Compute the missing percentages\ncolumns_to_drop = list(missing_percent[missing_percent > missing_threshold].index)\ndf = df.drop(columns=columns_to_drop, errors='ignore')  # Add errors='ignore'\n\n# Safely fill missing numerical values with the median\nnumerical_columns = df.select_dtypes(include=['number']).columns\nfor col in numerical_columns:\n    # Check for NaN values *before* filling\n    if df[col].isna().any().compute():\n        df[col] = df[col].fillna(df[col].median_approximate())\n    else:\n        print(f\"Column '{col}' has no missing values.\")\n\n# Fill categorical columns with mode safely\nif \"Gender\" in df.columns:\n    # Check for NaN before filling\n    if df[\"Gender\"].isna().any().compute():\n        gender_mode = df[\"Gender\"].mode().compute()[0]  # Compute mode before applying\n        df[\"Gender\"] = df[\"Gender\"].fillna(gender_mode)\n    else:\n        print(\"No missing values in Gender column.\")\n\n# Convert 'Hour' to numeric safely *before* doing anything else with it.\nif 'Hour' in df.columns:\n    df['Hour'] = df['Hour'].astype(str)  # Convert to string first\n    df['Hour'] = df['Hour'].str.replace(r'[^\\d\\.]', '', regex=True)  # remove non-numeric chars\n    df['Hour'] = dd.to_numeric(df['Hour'], errors='coerce')  # Convert to numeric, NaNs for errors\n\n    #DEBUG: See if you have a lot of NaN in `Hour`.\n    num_nans_hour = df['Hour'].isnull().sum().compute()\n    print(f\"Number of NaN values in 'Hour' column: {num_nans_hour}\")\n\n    # Drop NaN *after* converting to number (to handle error values)\n    df = df.dropna(subset=['Hour']) #drop across dask not compute()\n\n    # Now calculate features *after* cleaning the Hour feature, still in dask.\n    df['ElapsedTimeInDays'] = df['Hour'] // 24\n    df['Hour'] = df['Hour'] % 24  # Bring it back to the 0-23 range\nelse:\n    print(\"Warning: Column 'Hour' is missing from DataFrame.\")\n\n# Remove duplicate column names, run *before* computing the dataframe\ndf = df.loc[:, ~df.columns.duplicated()]\n\n# Drop irrelevant columns\n#This NEEDS to have patient id\ndf = df.drop(columns=['Unnamed: 0'], errors='ignore') # Add errors='ignore'\n\n# Compute the Dask DataFrame\ndf = df.compute()\n\n# %% [markdown]\n# # Data Splitting and SMOTE\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:53.276960Z\",\"iopub.execute_input\":\"2025-03-01T05:14:53.277300Z\",\"iopub.status.idle\":\"2025-03-01T05:14:55.208238Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:53.277275Z\",\"shell.execute_reply\":\"2025-03-01T05:14:55.207402Z\"}}\n# Cell 4: Data Splitting and SMOTE\n\n# Split into X and y\nX = df.drop(columns=['SepsisLabel', 'Patient_ID'])  #Important: Remove 'Patient_ID' for modeling\ny = df['SepsisLabel']\n\n#Now create the train and test dataset *before* over sampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Balance the data with SMOTE (only on the training data)\ntry:\n    smote = SMOTE(sampling_strategy=0.2, random_state=42)\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n    print(\"Data balanced with SMOTE.\")\n\n    # Convert resampled data back to Pandas DataFrame (Very important to convert back. We need to do KNN Imputation, and it must be a Pandas Dataframe)\n    X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n    y_train_resampled_df = pd.Series(y_train_resampled, name='SepsisLabel')\nexcept ImportError:\n    print(\"Warning: imblearn library not found. SMOTE cannot be applied.\")\nexcept Exception as e:\n    print(f\"Error during SMOTE: {e}\")\n\n# %% [markdown]\n# # Feature Engineering and KNN Imputation\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:55.209347Z\",\"iopub.execute_input\":\"2025-03-01T05:14:55.209595Z\",\"iopub.status.idle\":\"2025-03-01T05:14:56.261992Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:55.209575Z\",\"shell.execute_reply\":\"2025-03-01T05:14:56.261064Z\"}}\n# Cell 5: Feature Engineering and KNN Imputation\n\n#Apply lagged features on the resampled training data\nX_train_resampled_df = create_lagged_features(X_train_resampled_df, 'O2Sat', [1, 2, 3])\nX_train_resampled_df = create_lagged_features(X_train_resampled_df, 'FiO2', [1, 2, 3])\n\n#Also apply this to the test data\nX_test = create_lagged_features(X_test, 'O2Sat', [1, 2, 3])\nX_test = create_lagged_features(X_test, 'FiO2', [1, 2, 3])\n\n# KNN Imputation for train and test dataset\nX_train_resampled_df = safe_knn_impute(X_train_resampled_df)\nX_test = safe_knn_impute(X_test)\n\n# %% [markdown]\n# # Define Pipeline and Parameter Grid\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:56.262829Z\",\"iopub.execute_input\":\"2025-03-01T05:14:56.263127Z\",\"iopub.status.idle\":\"2025-03-01T05:14:56.267923Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:56.263099Z\",\"shell.execute_reply\":\"2025-03-01T05:14:56.266871Z\"}}\n# Cell 6: Define Pipeline and Parameter Grid\n\n# ==================== Now Modeling ======================\n# Define the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('selector', SelectKBest(score_func=f_classif)),\n    ('model', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n])\n\n# Set up parameter grid\nparam_grid = {\n    'selector__k': [5, 10, 15],\n    'model__learning_rate': [0.01, 0.1],\n    'model__n_estimators': [100, 200]\n}\n\n# %% [markdown]\n# # Initialize TimeSeriesSplit and GridSearchCV\n# \n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:56.268709Z\",\"iopub.execute_input\":\"2025-03-01T05:14:56.268976Z\",\"iopub.status.idle\":\"2025-03-01T05:14:56.283417Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:56.268956Z\",\"shell.execute_reply\":\"2025-03-01T05:14:56.282779Z\"}}\n# Cell 7: Initialize TimeSeriesSplit and GridSearchCV\n# Initialize TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5)\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=tscv, error_score='raise') # Set the error_score to 'raise' to debug any errors\n\n# %% [markdown]\n# # Fit the Model and Extract Best Parameters\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:14:56.284072Z\",\"iopub.execute_input\":\"2025-03-01T05:14:56.284299Z\",\"iopub.status.idle\":\"2025-03-01T05:19:06.201430Z\",\"shell.execute_reply.started\":\"2025-03-01T05:14:56.284281Z\",\"shell.execute_reply\":\"2025-03-01T05:19:06.199537Z\"}}\n# Cell 8: Fit the Model and Extract Best Parameters\n\n# Fit the model on the training data\ngrid_search.fit(X_train_resampled_df, y_train_resampled_df)\n\n# Extract the best model\nbest_model = grid_search.best_estimator_\nprint(\"Best parameters found: \", grid_search.best_params_)\n\n# %% [markdown]\n# # Calibrate the Model and Make Predictions\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:19:06.202177Z\",\"iopub.execute_input\":\"2025-03-01T05:19:06.202422Z\",\"iopub.status.idle\":\"2025-03-01T05:19:36.499010Z\",\"shell.execute_reply.started\":\"2025-03-01T05:19:06.202402Z\",\"shell.execute_reply\":\"2025-03-01T05:19:36.498342Z\"}}\n# Cell 9: Calibrate the Model and Make Predictions\n\n# Calibrate the best model\ncalibrated_model = CalibratedClassifierCV(best_model, method='isotonic', cv=5)\ncalibrated_model.fit(X_train_resampled_df, y_train_resampled_df)\n\n# Make predictions on the test data\ny_probs = calibrated_model.predict_proba(X_test)[:, 1]\n\n# %% [markdown]\n# # Optimize Threshold and Print Classification Report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:19:36.499913Z\",\"iopub.execute_input\":\"2025-03-01T05:19:36.500159Z\",\"iopub.status.idle\":\"2025-03-01T05:19:37.318169Z\",\"shell.execute_reply.started\":\"2025-03-01T05:19:36.500139Z\",\"shell.execute_reply\":\"2025-03-01T05:19:37.317265Z\"}}\n# Cell 10: Optimize Threshold and Print Classification Report\n\n# Optimize the prediction threshold\nbest_threshold = optimize_threshold(y_test, y_probs)\ny_pred = (y_probs > best_threshold).astype(int)\n\n# Print the classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# %% [markdown]\n# # Plot ROC Curve\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-03-01T05:19:37.319096Z\",\"iopub.execute_input\":\"2025-03-01T05:19:37.319437Z\",\"iopub.status.idle\":\"2025-03-01T05:19:37.780639Z\",\"shell.execute_reply.started\":\"2025-03-01T05:19:37.319407Z\",\"shell.execute_reply\":\"2025-03-01T05:19:37.779709Z\"}}\n# Cell 11: Plot ROC Curve\n\n# Plot ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_probs)\nroc_auc = roc_auc_score(y_test, y_probs)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nlogging.info(\"Script execution completed.\")","metadata":{"_uuid":"00405543-6875-4bcd-9322-4d568f02998b","_cell_guid":"4d505cd4-0f8a-4e3a-9f77-09b995d9530c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}